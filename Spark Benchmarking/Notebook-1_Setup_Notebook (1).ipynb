{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6407ac5e-b95a-4eb5-9588-4cb7336b203f",
   "metadata": {},
   "source": [
    "## Setup Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312be26e-46bc-4d6a-bb08-92c06beea5da",
   "metadata": {},
   "source": [
    "### Objective\n",
    "This notebook is used to configure DPGCE and Serverless environments for running benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc01d0e-76f7-4688-acf4-c89e6cd60777",
   "metadata": {},
   "source": [
    "### Initialize Public GCS Bucket\n",
    "This initializes the public GCS bucket from which datasets and utilities will be copied to your GCS bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40c1bcb1-ed7b-4f53-9a73-5bdbb401f64e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SOURCE_PUBLIC_GCS_BUCKET = \"tpc-benchmarking-kit-bucket\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38224fc-bbcd-4fab-80b0-e48f4af4cea5",
   "metadata": {},
   "source": [
    "### Setup Requirements\n",
    "Select the type of setup for benchmarking. By default, both DPGCE and Serverless environments are configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e50b2f2a-762d-47fc-be79-815ff60054dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataproc_setup_preferences = {\n",
    "    \"DPGCE_Standard\":True,\n",
    "    \"DPGCE_Premium\":True,\n",
    "    \"Serverless_Standard\":True,\n",
    "    \"Serverless_Premium\":True,\n",
    "    \"Serverless_Premium_with_NQE\":True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25ca728-06ad-424f-805a-f7dcc27366ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea564c59-d6db-4f5a-a008-c2875daf925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import yaml\n",
    "\n",
    "# Returns name of service account\n",
    "def get_gcloud_account():\n",
    "    try:\n",
    "        result = subprocess.run(['gcloud', 'config', 'get-value', 'account'],capture_output=True, text=True, check=True)\n",
    "        account = result.stdout.strip()\n",
    "        return account\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error running gcloud config get account: {e}\")\n",
    "        print(f\"{e.stderr}\")\n",
    "        return None\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: gcloud command not found. Please ensure the Google Cloud CLI is installed and in your system's PATH.\")\n",
    "        return None\n",
    "\n",
    "# Returns name of current project\n",
    "def get_gcloud_project():\n",
    "    try:\n",
    "        command = [\"gcloud\", \"config\", \"get\", \"project\"]\n",
    "        process = subprocess.run(command, capture_output=True, text=True, check=True)\n",
    "        project_name = process.stdout.strip()\n",
    "        return project_name\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error getting project name: {e}\")\n",
    "        print(f\"Stdout: {e.stdout}\")\n",
    "        print(f\"{e.stderr}\")\n",
    "        return None\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: gcloud command not found. Make sure the Google Cloud CLI is installed and in your system's PATH.\")\n",
    "        return None\n",
    "    \n",
    "# Creates a Dataproc cluster    \n",
    "def create_cluster(cluster_configs: dict):\n",
    "    command = f\"\"\" ! gcloud dataproc clusters create {cluster_configs[\"dp_cluster_name\"]} \\\n",
    "      --region {cluster_configs[\"region\"]} \\\n",
    "      --tier {cluster_configs[\"tier\"]} \\\n",
    "      --scopes cloud-platform \\\n",
    "      --enable-component-gateway \\\n",
    "      --no-address \\\n",
    "      --service-account {cluster_configs[\"service_account\"]} \\\n",
    "      --subnet {cluster_configs[\"subnet\"]} \\\n",
    "      --num-masters {cluster_configs[\"num_master\"]} \\\n",
    "      --num-workers {cluster_configs[\"num_worker\"]} \\\n",
    "      --master-machine-type {cluster_configs[\"master_vm\"]} \\\n",
    "      --master-boot-disk-size {cluster_configs[\"master_boot_disk_size\"]} \\\n",
    "      --master-boot-disk-type {cluster_configs[\"master_boot_disk_type\"]} \\\n",
    "      --worker-machine-type {cluster_configs[\"worker_vm\"]} \\\n",
    "      --worker-boot-disk-type {cluster_configs[\"worker_boot_disk_type\"]} \\\n",
    "      --worker-boot-disk-size {cluster_configs[\"worker_boot_disk_size\"]} \\\n",
    "      --num-master-local-ssds {cluster_configs[\"master_ssd_count\"]} \\\n",
    "      --num-worker-local-ssds {cluster_configs[\"worker_ssd_count\"]} \\\n",
    "      --master-local-ssd-interface {cluster_configs[\"master_local_ssd_interface\"]} \\\n",
    "      --worker-local-ssd-interface {cluster_configs[\"worker_local_ssd_interface\"]} \\\n",
    "      --image-version {cluster_configs[\"image_version\"]} \\\n",
    "      --properties \"hive:yarn.log-aggregation-enable=true\" \\\n",
    "      --properties \"spark:spark.checkpoint.compress=true\" \\\n",
    "      --properties \"spark:spark.eventLog.compress=true\" \\\n",
    "      --properties \"spark:spark.eventLog.compression.codec=zstd\" \\\n",
    "      --properties \"spark:spark.eventLog.rolling.enabled=true\" \\\n",
    "      --properties \"spark:spark.io.compression.codec=zstd\" \\\n",
    "      --properties \"spark:spark.sql.parquet.compression.codec=zstd\" \\\n",
    "      --optional-components=JUPYTER \\\n",
    "      \"\"\"\n",
    "    \n",
    "    print(command)\n",
    "    ! {command}\n",
    "    \n",
    "# Creates a Dataproc Serverless session template\n",
    "def create_template(yaml_file_path: str, tier: str, runtype: str, session_configs: dict):\n",
    "    command = [\n",
    "        \"gcloud\",\n",
    "        \"beta\",\n",
    "        \"dataproc\",\n",
    "        \"session-templates\",\n",
    "        \"import\",\n",
    "        f\"{tier}-{runtype}-runtime\",\n",
    "        \"--source\",\n",
    "        yaml_file_path,\n",
    "        \"--location\",\n",
    "        f\"{session_configs['region']}\",\n",
    "        \"--project\",\n",
    "        PROJECT,\n",
    "    ]\n",
    "\n",
    "    process = subprocess.Popen(command, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    stdout, stderr = process.communicate(input=\"y\\n\")  \n",
    "\n",
    "    print(stdout)\n",
    "    print(\"Stderr:\", stderr)\n",
    "    \n",
    "# Updates the default session template with the provided configurations\n",
    "def update_session_configs_in_template(file_path : str):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = yaml.safe_load(f)\n",
    "\n",
    "    data['environmentConfig']['executionConfig']['serviceAccount'] = session_configs[\"service_account\"]\n",
    "    data['environmentConfig']['executionConfig']['subnetworkUri'] = session_configs[\"subnet\"]\n",
    "    data['environmentConfig']['executionConfig']['idleTtl'] = session_configs[\"idleTTL\"]\n",
    "    data['environmentConfig']['executionConfig']['ttl'] = session_configs[\"TTL\"]\n",
    "    data['runtimeConfig']['version'] = session_configs[\"version\"]\n",
    "\n",
    "    with open(file_path, \"w\") as f:\n",
    "        yaml.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41cad641-4a02-4b8d-b380-52bb043fd9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: nishitkamdarargo\n",
      "Service Account: sparkpoc-sa@nishitkamdarargo.iam.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "# Get the value of current project and service account in use\n",
    "PROJECT = get_gcloud_project()\n",
    "SERVICE_ACCOUNT = get_gcloud_account()\n",
    "\n",
    "print(f\"Project: {PROJECT}\")\n",
    "print(f\"Service Account: {SERVICE_ACCOUNT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6e459e-cca0-480c-b0c0-cf94b8080198",
   "metadata": {},
   "source": [
    "### DPGCE Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453617b9-cc8a-4852-a68f-bd4367d83df8",
   "metadata": {},
   "source": [
    "#### Cluster Configuration\n",
    "A Dataproc cluster will be created using the default configurations below. \n",
    "\n",
    "You can customize these settings in the following code cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64547477-af8c-4b10-97ff-c260cd1a3e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default Configurations\n",
    "##############################################################################################################################\n",
    "# DP_CLUSTER_NAME=\"dpgce-benchmarking\"  \n",
    "# REGION=\"us-east1\"                  \n",
    "# MASTER_TYPE=\"c3d-standard-16-lssd\" \n",
    "# WORKER_TYPE=\"c3d-standard-16-lssd\" \n",
    "# WORKER_COUNT=\"10\"   [For 10TB benchmarks, it is recommended to increase this count to 27]                \n",
    "# WORKER_SSD_COUNT=\"1\"               \n",
    "# MASTER_BOOT_DISK_SIZE=\"500\"        \n",
    "# WORKER_BOOT_DISK_SIZE=\"500\"        \n",
    "# MASTER_LOCAL_SSDS=\"1\"              \n",
    "# MASTER_LOCAL_SSD_INTERFACE=\"NVME\"  \n",
    "# WORKER_LOCAL_SSD_INTERFACE=\"NVME\"  \n",
    "# IMAGE_VERSION=\"2.3-debian12\" (other supported versions include 2.2-debian12 and 2.1-debian11)\n",
    "# SUBNET=\"sparkpoc-subnet\"           \n",
    "##############################################################################################################################\n",
    "\n",
    "cluster_configs = {\n",
    "    \"dp_cluster_name\":\"dpgce-benchmarking\",\n",
    "    \"region\":\"us-east1\",\n",
    "    \"master_vm\":\"n2d-standard-16\",\n",
    "    \"num_master\":1,\n",
    "    \"master_ssd_count\":1,\n",
    "    \"master_boot_disk_size\":500,\n",
    "    \"master_local_ssd_interface\":\"NVME\",\n",
    "    \"master_boot_disk_type\":\"pd-balanced\",\n",
    "    \"worker_vm\":\"n2d-standard-16\",\n",
    "    \"num_worker\":10,\n",
    "    \"worker_ssd_count\":1,\n",
    "    \"worker_boot_disk_size\":500,\n",
    "    \"worker_local_ssd_interface\":\"NVME\",\n",
    "    \"worker_boot_disk_type\":\"pd-balanced\",\n",
    "    \"image_version\":\"2.3-debian12\",\n",
    "    \"subnet\":\"sparkpoc-subnet\",\n",
    "    \"service_account\": SERVICE_ACCOUNT,\n",
    "    \"tier\": \"standard\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c97cb92-49eb-4f04-8501-ba4eaca6c2ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### DPGCE Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e874ee-9cfb-4ebd-b842-da8efe47acc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataproc_setup_preferences[\"DPGCE_Standard\"]:\n",
    "    print(\"Setting up a DPGCE Standard cluster\")\n",
    "    cluster_configs[\"dp_cluster_name\"]=\"dpgce-benchmarking-standard\"\n",
    "    create_cluster(cluster_configs)\n",
    "else:\n",
    "    print(\"DPGCE Standard setup not required, skipping this step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d725b9a-07cb-4a29-b832-9a7fee4e0d20",
   "metadata": {},
   "source": [
    "#### DPGCE Premium with Lightning Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d11eab6-9c03-4a46-bed5-eda114d2a996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPGCE Premium with Lightning Engine is available only on image version 2.3-debian12\n",
    "\n",
    "if dataproc_setup_preferences[\"DPGCE_Premium\"]:\n",
    "    print(\"Setting up a DPGCE Premium cluster\")\n",
    "    cluster_configs[\"dp_cluster_name\"]=\"dpgce-benchmarking-premium-2\"\n",
    "    cluster_configs[\"tier\"]=\"premium\"\n",
    "    cluster_configs[\"image_version\"]=\"2.3.4-debian12\"\n",
    "    create_cluster(cluster_configs)\n",
    "else:\n",
    "    print(\"DPGCE Premium setup not required, skipping this step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbb6ac5-5fe5-4d9c-91e1-0b430639bc57",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataproc Serverless Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92d24ca-72af-4b2c-b5e7-823836208b56",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Session Configuration\n",
    "A Dataproc session template with the following specifications will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05f1174e-7d05-4cd1-86e3-b55f2f1b5934",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session_configs = {\n",
    "    \"region\":\"us-east1\",\n",
    "    \"version\":\"2.2\",\n",
    "    \"service_account\":SERVICE_ACCOUNT,\n",
    "    \"subnet\":\"sparkpoc-subnet\",  # Default subnet provisioned by Terraform. Update 'SUBNET' if using a different subnet.\n",
    "    \"idleTTL\":\"28800s\",\n",
    "    \"TTL\":\"86400s\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e68aaa3-11b1-445a-8428-994b26d7f4dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Serverless Standard\n",
    "\n",
    "Create Session Template for Serverless Standard\n",
    "\n",
    "**Important - After the template is created, locate the displayName in the output. This displayName will be the name of the kernel you use to run your Dataproc Serverless notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fac212e-d62b-489e-96ea-c1a2d2388854",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://tpc-benchmarking-kit-bucket/session_templates/s8s-standard-spark-runtime-template.yaml...\n",
      "/ [1 files][  957.0 B/  957.0 B]                                                \n",
      "Operation completed over 1 objects/957.0 B.                                      \n",
      "createTime: '2025-08-01T08:33:41.405630Z'\n",
      "creator: sparkpoc-sa@nishitkamdarargo.iam.gserviceaccount.com\n",
      "description: Run TPC benchmark on Serverless Standard\n",
      "environmentConfig:\n",
      "  executionConfig:\n",
      "    idleTtl: 28800s\n",
      "    serviceAccount: sparkpoc-sa@nishitkamdarargo.iam.gserviceaccount.com\n",
      "    subnetworkUri: sparkpoc-subnet\n",
      "    ttl: 86400s\n",
      "jupyterSession:\n",
      "  displayName: Standard for running TPC Benchmark\n",
      "  kernel: PYTHON\n",
      "labels:\n",
      "  client: bigquery-jupyter-plugin\n",
      "name: projects/nishitkamdarargo/locations/us-east1/sessionTemplates/standard-spark-runtime\n",
      "runtimeConfig:\n",
      "  properties:\n",
      "    spark.dataproc.driver.compute.tier: standard\n",
      "    spark.dataproc.driver.disk.size: 750g\n",
      "    spark.dataproc.driver.disk.tier: standard\n",
      "    spark.dataproc.executor.compute.tier: standard\n",
      "    spark.dataproc.executor.disk.size: 750g\n",
      "    spark.dataproc.executor.disk.tier: standard\n",
      "    spark.driver.cores: '16'\n",
      "    spark.driver.memory: 58g\n",
      "    spark.dynamicAllocation.enabled: 'false'\n",
      "    spark.executor.cores: '16'\n",
      "    spark.executor.instances: '9'\n",
      "    spark.executor.memory: 58g\n",
      "    spark.gluten.sql.columnar.backend.velox.glogSeverityLevel: '2'\n",
      "    spark.log.level: ERROR\n",
      "  version: '2.2'\n",
      "updateTime: '2025-08-07T10:42:55.063886Z'\n",
      "uuid: 8d936b1e-a0b1-4fd3-be87-bc19baca1c9f\n",
      "\n",
      "Stderr: Session template [projects/nishitkamdarargo/locations/us-east1/sessionTemplates/\n",
      "standard-spark-runtime] will be overwritten.\n",
      "\n",
      "Do you want to continue (Y/n)?  \n",
      "Updated [projects/nishitkamdarargo/locations/us-east1/sessionTemplates/standard-spark-runtime].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if dataproc_setup_preferences[\"Serverless_Standard\"]:\n",
    "    \n",
    "    # Copy the session template file\n",
    "    ! gsutil cp \"gs://{SOURCE_PUBLIC_GCS_BUCKET}/session_templates/s8s-standard-spark-runtime-template.yaml\" .\n",
    "\n",
    "    # Update the template with the provided session configs\n",
    "    update_session_configs_in_template('./s8s-standard-spark-runtime-template.yaml')\n",
    "    \n",
    "    # Create session template\n",
    "    create_template('./s8s-standard-spark-runtime-template.yaml','standard','spark',session_configs)    \n",
    "else:\n",
    "    print(\"Serverless Standard setup not required, skipping this step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dea8cc0-c0ae-4952-af44-b9cac7aa504a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Serverless Premium\n",
    "\n",
    "Create Session Template for Serverless Premium\n",
    "\n",
    "**Important - After the template is created, locate the displayName in the output. This displayName will be the name of the kernel you use to run your Dataproc Serverless notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd3c4b5-0817-44ff-b0af-65208da844f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if dataproc_setup_preferences[\"Serverless_Premium\"]:\n",
    "    \n",
    "    # Copy the session template file\n",
    "    ! gsutil cp \"gs://{SOURCE_PUBLIC_GCS_BUCKET}/session_templates/s8s-premium-spark-runtime-template.yaml\" .\n",
    "\n",
    "    # Update the template with the provided session configs\n",
    "    update_session_configs_in_template('./s8s-premium-spark-runtime-template.yaml')\n",
    "    \n",
    "    # Create session template\n",
    "    create_template('./s8s-premium-spark-runtime-template.yaml','premium','spark',session_configs)    \n",
    "else:\n",
    "    print(\"Serverless Premium setup not required, skipping this step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc72c19-1db8-4e8c-af25-33ac30caf115",
   "metadata": {},
   "source": [
    "#### Serverless Premium with NQE\n",
    "\n",
    "Create Session Template for Serverless Premium with NQE\n",
    "\n",
    "**Important - After the template is created, locate the displayName in the output. This displayName will be the name of the kernel you use to run your Dataproc Serverless notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35b1095-08f7-4eeb-b6de-5f5c4e7ad98d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if dataproc_setup_preferences[\"Serverless_Premium_with_NQE\"]:\n",
    "    \n",
    "    # Copy the session template file\n",
    "    ! gsutil cp \"gs://{SOURCE_PUBLIC_GCS_BUCKET}/session_templates/s8s-premium-native-runtime-template.yaml\" .\n",
    "\n",
    "    # Update the template with the provided session configs\n",
    "    update_session_configs_in_template('./s8s-premium-native-runtime-template.yaml')\n",
    "    \n",
    "    # Create session template    \n",
    "    create_template('./s8s-premium-native-runtime-template.yaml','premium','native',session_configs)\n",
    "else:\n",
    "    print(\"Serverless Premium with NQE setup not required, skipping this step.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73005800-f6e4-4a53-987f-a50c1d7308e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
