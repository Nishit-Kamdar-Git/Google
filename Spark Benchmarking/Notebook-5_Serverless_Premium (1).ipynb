{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc2eec9c-7fe4-48d5-a6e1-7f48702e85a9",
   "metadata": {},
   "source": [
    "## Serverless Premium without NQE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266f5aa7-2fce-4d87-ae4c-438e8ec0a0c4",
   "metadata": {},
   "source": [
    "### Objective\n",
    "This notebook is designed for running TPCH and TPCDS benchmarks on Serverless Premium using Serverless Interactive sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee703db2-ff94-416e-83f0-3f85cb432b06",
   "metadata": {},
   "source": [
    "### Initialize Public GCS Bucket\n",
    "This initializes the public GCS bucket from which datasets and utilities will be copied to your GCS bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dead9ed-2124-4cb5-9918-d79873cb6fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_PUBLIC_GCS_BUCKET = \"tpc-benchmarking-kit-bucket\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50f2aeb-0bf7-47e7-ab54-79646e501bab",
   "metadata": {},
   "source": [
    "### Initialization Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04c2a507-4082-4270-9185-9e0f9bafaf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Google Cloud Project:nishitkamdarargo\n",
      "Current Project Number: 279814974066\n",
      "Current Region: us-east1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import subprocess\n",
    "from typing import Tuple\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Returns name of current project\n",
    "def get_gcloud_project():\n",
    "    try:\n",
    "        command = [\"gcloud\", \"config\", \"get\", \"project\"]\n",
    "        process = subprocess.run(command, capture_output=True, text=True, check=True)\n",
    "        project_name = process.stdout.strip()\n",
    "        return project_name\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        raise ValueError(f\"Error getting gcloud project: {e}\\nStdout: {e.stdout}\\nStderr: {e.stderr}\")\n",
    "    except FileNotFoundError:\n",
    "        raise ValueError(\"Error: gcloud command not found. Make sure the Google Cloud CLI is installed and in your system's PATH.\")\n",
    "\n",
    "# Retrieves the current region in use\n",
    "def get_gcloud_region():\n",
    "    metadata_url = \"http://metadata.google.internal/computeMetadata/v1/instance/zone\"\n",
    "    headers = {\"Metadata-Flavor\": \"Google\"}\n",
    "\n",
    "    try:\n",
    "        response = requests.get(metadata_url, headers=headers, timeout=5)\n",
    "        response.raise_for_status() \n",
    "        zone = response.text.split('/')[-1]\n",
    "        region = '-'.join(zone.split('-')[:-1])\n",
    "        return region\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Could not retrieve region from metadata server: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Returns project number of current project\n",
    "def get_project_number(project_id: str) -> str:\n",
    "    try:\n",
    "        command = [\n",
    "            \"gcloud\",\n",
    "            \"projects\",\n",
    "            \"describe\",\n",
    "            project_id,\n",
    "            \"--format=value(projectNumber)\",\n",
    "        ]\n",
    "        result = subprocess.run(command, capture_output=True, text=True, check=True)\n",
    "        project_number = result.stdout.strip()\n",
    "        return project_number\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        raise ValueError(f\"Error executing gcloud command for project ID '{project_id}': {e}\\nStderr: {e.stderr}\")\n",
    "    except FileNotFoundError:\n",
    "        raise ValueError(\"Error: gcloud CLI not found. Make sure it's installed and in your system's PATH.\")\n",
    "        \n",
    "PROJECT = get_gcloud_project()\n",
    "PROJECT_NUMBER = get_project_number(PROJECT)\n",
    "REGION = get_gcloud_region()\n",
    "TIER = \"premium\"\n",
    "RUNTYPE = \"spark\"\n",
    "\n",
    "TPCH_TABLES = [\"customer\", \"lineitem\", \"nation\", \"orders\", \"part\", \"partsupp\", \"region\", \"supplier\"]\n",
    "TPCDS_TABLES = [\"call_center\", \"catalog_page\", \"catalog_returns\", \"catalog_sales\", \"customer\", \"customer_address\", \"customer_demographics\", \"date_dim\", \"household_demographics\", \"income_band\", \"inventory\", \"item\", \"promotion\", \"reason\", \"ship_mode\", \"store\", \"store_returns\", \"store_sales\", \"time_dim\", \"warehouse\", \"web_page\", \"web_returns\", \"web_sales\", \"web_site\"]\n",
    "\n",
    "print(f\"Current Google Cloud Project:{PROJECT}\")\n",
    "print(f\"Current Project Number: {PROJECT_NUMBER}\")\n",
    "print(f\"Current Region: {REGION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11734fb5-cf7d-4364-a4ba-65ec74bb49a8",
   "metadata": {},
   "source": [
    "### Initialize your GCS Bucket\n",
    "To run the benchmarks, a Google Cloud Storage (GCS) bucket is required to store necessary utilities.\n",
    "\n",
    "The default GCS bucket name is `sparkpoc-bucket-serverless-premium-PROJECT_NUMBER-REGION`. You can use a different bucket name as needed.\n",
    "\n",
    "\n",
    "**NOTE - The following cell will attempt to create a storage bucket. If a bucket with this name already exists in the specified region and you own it, you might see a \"bucket name taken\" message or a confirmation of a previous creation. You can safely ignore these messages if your desired bucket is already created. Otherwise, please choose a unique bucket name and try again.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "244860df-7ee1-4109-ab6f-9189c3893710",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCS Bucket Name: sparkpoc-bucket-serverless-premium-279814974066-us-east1\n"
     ]
    }
   ],
   "source": [
    "BUCKET_NAME = f\"sparkpoc-bucket-serverless-premium-{PROJECT_NUMBER}-{REGION}\"\n",
    "print(f\"GCS Bucket Name: {BUCKET_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4855263b-434b-4ad7-addd-25db53bb981b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://sparkpoc-bucket-serverless-premium-279814974066-us-east1/...\n"
     ]
    }
   ],
   "source": [
    "! gcloud storage buckets create gs://{BUCKET_NAME} --uniform-bucket-level-access --location={REGION}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767787aa-bb6c-4274-b7a6-0df96b5ce12d",
   "metadata": {},
   "source": [
    "### Benchmark Configuration Setup\n",
    "\n",
    "This step initializes the benchmark configurations. These configurations define the parameters for the benchmark run, such as:\n",
    "\n",
    "- **Benchmark:** The type of benchmark to run (e.g., \"tpch\",  \"tpcds\",  \"tpch,tpcds\",  \"tpcds,tpch\").\n",
    "- **TPCH Queries to Run:** The specific queries to run for TPCH (e.g., \"q1,q2\", \"q3\", \"all\").\n",
    "- **TPCDS Queries to Run:** The specific queries to run for TPCDS (e.g., \"q1,q2\", \"q3\", \"all\").\n",
    "- **Size:** The scale factor or data size (e.g., \"sf1000\", \"sf10000\").\n",
    "- **File Format:** The currently supported file format is Parquet.\n",
    "\n",
    "**NOTE: You can customize the benchmark_configs to adjust the benchmark settings. After making changes, restart the kernel and re-run all cells for the modifications to take effect.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6b63624-845f-479c-8857-d1274341565f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "benchmark_configs = {\n",
    "    \"benchmark\":\"tpch,tpcds\",      # the benchmark that gets specified first is run first\n",
    "    \"TPCH_queries_to_run\": \"all\",  # if benchmark does not contain tpch, this will be ignored\n",
    "    \"TPCDS_queries_to_run\": \"all\", # if benchmark does not contain tpcds, this will be ignored\n",
    "    \"size\": \"sf1000\",              # sf1000 for 1000GB, sf10000 for 10000GB\n",
    "    \"file_format\": \"parquet\" \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e6c013a-213f-406c-af21-c374e6b1d569",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_string = benchmark_configs.get(\"benchmark\", \"\")\n",
    "file_format = benchmark_configs.get(\"file_format\", \"\")\n",
    "size = benchmark_configs.get(\"size\",\"\")\n",
    "\n",
    "benchmarks_list = [bench.strip() for bench in benchmark_string.split(',')]\n",
    "\n",
    "first_benchmark = benchmarks_list[0] if len(benchmarks_list) > 0 else None\n",
    "second_benchmark = benchmarks_list[1] if len(benchmarks_list) > 1 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783233ba-68f8-4a0f-bb30-faa73d26010d",
   "metadata": {},
   "source": [
    "### Validating Benchmark Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2a4aac2-1321-4c1e-b749-70fc051f850a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark Configurations\n",
      "First benchmark: tpch\n",
      "Second benchmark: tpcds\n",
      "Scale Factor: sf1000\n",
      "File Format: parquet\n"
     ]
    }
   ],
   "source": [
    "# Check for valid benchmark types\n",
    "\n",
    "if first_benchmark is None:\n",
    "    print(\"Error! Please specify benchmark to be run.\")\n",
    "    print(\"Valid benchmark types are - \\n 1. tpch \\n 2. tpcds \\n 3. tpch,tpcds \\n 4. tpcds, tpch\")\n",
    "    raise SystemExit()\n",
    "    \n",
    "if first_benchmark != \"tpch\" and first_benchmark != \"tpcds\":\n",
    "    print(f\"Error! Unrecognised benchmark type {first_benchmark}.\")\n",
    "    print(\"Valid benchmark types are - \\n 1. tpch \\n 2. tpcds \\n 3. tpch,tpcds \\n 4. tpcds,tpch\")\n",
    "    raise SystemExit()\n",
    "    \n",
    "if second_benchmark is not None and second_benchmark != \"tpch\" and second_benchmark != \"tpcds\":\n",
    "    print(f\"Error! Unrecognised benchmark type {second_benchmark}.\")\n",
    "    print(\"Valid benchmark types are - \\n 1. tpch \\n 2. tpcds \\n 3. tpch,tpcds \\n 4. tpcds,tpch\")\n",
    "    raise SystemExit()\n",
    "    \n",
    "# Check for valid scale factor\n",
    "if size is None or size == \"\" :\n",
    "    print(\"Error! Please specify scale factor/size\")\n",
    "    print(\"Valid size values are - \\n 1. sf1000 \\n 2. sf10000\")\n",
    "    raise SystemExit()\n",
    "\n",
    "if size != \"sf1000\" and size != \"sf10000\":\n",
    "    print(f\"Error! Unsupported value for size {size}\")\n",
    "    print(\"Valid size values are - \\n 1. sf1000 \\n 2. sf10000\")\n",
    "    raise SystemExit()\n",
    "\n",
    "# Check for valid file formats\n",
    "if file_format is None or file_format == \"\":\n",
    "    print(\"Error! Please specify file format\")\n",
    "    print(\"Valid file formats are - \\n 1. parquet\")\n",
    "    raise SystemExit()    \n",
    "\n",
    "if file_format != \"parquet\":\n",
    "    print(f\"Error! Unrecognised file format {file_format}\")\n",
    "    print(\"Valid file formats are - \\n 1. parquet\")\n",
    "    raise SystemExit()\n",
    "      \n",
    "\n",
    "print(\"Benchmark Configurations\")\n",
    "print(f\"First benchmark: {first_benchmark}\")\n",
    "print(f\"Second benchmark: {second_benchmark}\")\n",
    "print(f\"Scale Factor: {size}\")\n",
    "print(f\"File Format: {file_format}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422220b1-6e43-426e-b589-6e992f6adbf1",
   "metadata": {},
   "source": [
    "### Defining the Run Id\n",
    "The following cell defines a unique identifier for the current notebook execution. This Run Id can be used in the `Comparison_Notebook` for Runtime and Cost performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd01af6e-4390-4cf0-a954-db4ae5e21c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Run Id for current notebook session is serverless_premium_spark_parquet_sf1000_tpch_tpcds_us-east1_2025-08-08_09-08-18\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "now_utc = datetime.now(timezone.utc)\n",
    "timestamp_str = now_utc.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "RUN_ID = f\"serverless_{TIER}_{RUNTYPE}_{file_format}_{size}_{first_benchmark}\"\n",
    "\n",
    "if second_benchmark is not None:\n",
    "    RUN_ID+=f\"_{second_benchmark}\"\n",
    "\n",
    "RUN_ID+=f\"_{REGION}_{timestamp_str}\"\n",
    "print(f\"The Run Id for current notebook session is {RUN_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35801f78-e421-411b-b962-96a50a534859",
   "metadata": {},
   "source": [
    "### Copy Utilities\n",
    "\n",
    "Run the following cells to copy the necessary utilities in your GCS bucket. \n",
    "\n",
    "Duplicate file copy operations will be skipped.\n",
    "\n",
    "**Troubleshooting: If the copy operation seems to hang at 100% for a significant time, it's likely due to a temporary error. In such cases, restarting the kernel and running all cells from the beginning is recommended.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25c54bf7-8a94-4d84-987b-68dedf67a6d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying utilities\n",
      "Source GCS location: gs://tpc-benchmarking-kit-bucket/notebook_utils/\n",
      "Destination GCS location: gs://sparkpoc-bucket-serverless-premium-279814974066-us-east1/premium/spark/notebook_utils/\n",
      "\n",
      "Polling for latest operation name...done.                                      \n",
      "Operation name: transferJobs-3744368142147906012-12294480453413668230\n",
      "Parent job: 3744368142147906012\n",
      "Start time: 2025-08-08T09:08:20.326578546Z\n",
      "SUCCESS | 100% (162.6kiB of 162.6kiB) | Skipped: 0B | Errors: 0 : 0 -\n",
      "End time: 2025-08-08T09:08:43.484708861Z\n"
     ]
    }
   ],
   "source": [
    "source_location = f\"gs://{SOURCE_PUBLIC_GCS_BUCKET}/notebook_utils/\"\n",
    "destination_location = f\"gs://{BUCKET_NAME}/{TIER}/{RUNTYPE}/notebook_utils/\"\n",
    "\n",
    "print(\"Copying utilities\")\n",
    "print(f\"Source GCS location: {source_location}\")\n",
    "print(f\"Destination GCS location: {destination_location}\")\n",
    "print(\"\")\n",
    "\n",
    "# Copy notebook_utils which contains queries\n",
    "! gcloud transfer jobs monitor \\\n",
    "$(gcloud transfer jobs create \\\n",
    "  --format='value(name)' \\\n",
    "  \"{source_location}\" \"{destination_location}\"\\\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5236050-b976-4025-bcbc-391f13829c55",
   "metadata": {},
   "source": [
    "### Copy Datasets\n",
    "\n",
    "Duplicate file copy operations will be skipped.\n",
    "\n",
    "**Troubleshooting: If the copy operation seems to hang at 100% for a significant time, it's likely due to a temporary error. In such cases, restarting the kernel and running all cells from the beginning is recommended.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87a060c5-3178-4259-8e2a-9fc929bc58a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying tpch sf1000 datasets\n",
      "Source GCS location: gs://tpc-benchmarking-kit-bucket/tpch/sf1000/\n",
      "Destination GCS location: gs://sparkpoc-bucket-serverless-premium-279814974066-us-east1/premium/spark/tpch/sf1000/\n",
      "\n",
      "Polling for latest operation name...done.                                      \n",
      "Operation name: transferJobs-3041358676953755158-10912848747178414418\n",
      "Parent job: 3041358676953755158\n",
      "Start time: 2025-08-08T09:08:46.022275281Z\n",
      "SUCCESS | 100% (351.5GiB of 351.5GiB) | Skipped: 0B | Errors: 0 : 0 /\n",
      "End time: 2025-08-08T09:10:12.951013058Z\n"
     ]
    }
   ],
   "source": [
    "print(f\"Copying {first_benchmark} {size} datasets\")\n",
    "\n",
    "source_location = f\"gs://{SOURCE_PUBLIC_GCS_BUCKET}/{first_benchmark}/{size}/\"\n",
    "destination_location = f\"gs://{BUCKET_NAME}/{TIER}/{RUNTYPE}/{first_benchmark}/{size}/\"\n",
    "\n",
    "print(f\"Source GCS location: {source_location}\")\n",
    "print(f\"Destination GCS location: {destination_location}\")\n",
    "print(\"\")\n",
    "\n",
    "# Copy TPC datasets\n",
    "! gcloud transfer jobs monitor \\\n",
    "$(gcloud transfer jobs create \\\n",
    "  --format='value(name)' \\\n",
    "  \"{source_location}\" \"{destination_location}\"\\\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b67a6806-96ee-4d4d-a843-bcb695324805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying tpcds sf1000 datasets\n",
      "Source GCS location: gs://tpc-benchmarking-kit-bucket/tpcds/sf1000/\n",
      "Destination GCS location: gs://sparkpoc-bucket-serverless-premium-279814974066-us-east1/premium/spark/tpcds/sf1000/\n",
      "\n",
      "Polling for latest operation name...done.                                      \n",
      "Operation name: transferJobs-17145946592680002862-7687083134840480845\n",
      "Parent job: 17145946592680002862\n",
      "Start time: 2025-08-08T09:10:16.018322779Z\n",
      "SUCCESS | 100% (304.3GiB of 304.3GiB) | Skipped: 0B | Errors: 0 : 0 |\n",
      "End time: 2025-08-08T09:13:20.748453142Z\n"
     ]
    }
   ],
   "source": [
    "if second_benchmark is not None:\n",
    "    print(f\"Copying {second_benchmark} {size} datasets\")\n",
    "\n",
    "    source_location = f\"gs://{SOURCE_PUBLIC_GCS_BUCKET}/{second_benchmark}/{size}/\"\n",
    "    destination_location = f\"gs://{BUCKET_NAME}/{TIER}/{RUNTYPE}/{second_benchmark}/{size}/\"\n",
    "\n",
    "    print(f\"Source GCS location: {source_location}\")\n",
    "    print(f\"Destination GCS location: {destination_location}\")\n",
    "    print(\"\")    \n",
    "    # Copy TPC datasets\n",
    "    ! gcloud transfer jobs monitor \\\n",
    "    $(gcloud transfer jobs create \\\n",
    "      --format='value(name)' \\\n",
    "      \"{source_location}\" \"{destination_location}\"\\\n",
    "    )\n",
    "else:\n",
    "    print(\"Second benchmark is empty. Skipping this step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aef812-5973-4939-a97d-3a453ca23d49",
   "metadata": {},
   "source": [
    "### Define Benchmark Utility Functions\n",
    "\n",
    "To facilitate the execution of TPC benchmarks, we need to load a set of utility functions. These functions are defined in the following cell and handle tasks such as:\n",
    "\n",
    "- Query execution timing\n",
    "- Data loading and setup\n",
    "- Result formatting\n",
    "- Writing results to GCS\n",
    "- Terminating current Serverless session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e0c10af-7a47-408b-ac89-08161cb2f79b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from urllib.parse import urlparse\n",
    "from py4j.java_gateway import java_import\n",
    "import os\n",
    "from google.cloud import storage\n",
    "import time\n",
    "from typing import Callable, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "from google.cloud import dataproc_v1 as dataproc\n",
    "import json\n",
    "import subprocess\n",
    "import re\n",
    "import csv\n",
    "import io\n",
    "\n",
    "################################################ Run Benchmark ############################################\n",
    "\n",
    "# Set up TPCH tables\n",
    "def setupTPCHTables(spark: SparkSession, locationPath: str, size: str):\n",
    "    spark.sparkContext.setJobDescription(\"SETUP\")\n",
    "    tpchtables = [\"customer\", \"lineitem\", \"nation\", \"orders\", \"part\", \"partsupp\", \"region\", \"supplier\"]\n",
    "\n",
    "    print(\"Setting up TPCH tables\")\n",
    "\n",
    "    ntn = f\"\"\"create table nation (n_nationkey long, n_name string, n_regionkey long, n_comment string) using parquet location \"{locationPath}/tpch/{size}/parquet/partitioned/{size}_parquet/nation/\" options (\"compression\"=\"snappy\")\"\"\"\n",
    "    rgn = f\"\"\"create table region (r_regionkey long, r_name string, r_comment string) using parquet location \"{locationPath}/tpch/{size}/parquet/partitioned/{size}_parquet/region/\" options (\"compression\"=\"snappy\")\"\"\"\n",
    "    prtsupp = f\"\"\"create table partsupp (ps_partkey long, ps_suppkey long, ps_availqty int, ps_supplycost decimal(12, 2), ps_comment string) using parquet location \"{locationPath}/tpch/{size}/parquet/partitioned/{size}_parquet/partsupp/\" options (\"compression\"=\"snappy\")\"\"\"\n",
    "    supp = f\"\"\"create table supplier (s_suppkey long, s_name string, s_address string, s_nationkey long, s_phone string, s_acctbal decimal(12, 2), s_comment string) using parquet location \"{locationPath}/tpch/{size}/parquet/partitioned/{size}_parquet/supplier/\" options (\"compression\"=\"snappy\")\"\"\"\n",
    "    odrs = f\"\"\"create table orders (o_orderkey long, o_custkey long, o_orderstatus string, o_totalprice decimal(12, 2), o_orderdate date, o_orderpriority string, o_clerk string, o_shippriority int, o_comment string) using parquet partitioned by (o_orderdate) location \"{locationPath}/tpch/{size}/parquet/partitioned/{size}_parquet/orders/\" options (\"compression\"=\"snappy\")\"\"\"\n",
    "    cstmr = f\"\"\"create table customer (c_custkey long, c_name string, c_address string, c_nationkey long, c_phone string, c_acctbal decimal(12, 2), c_mktsegment string, c_comment string) using parquet partitioned by (c_mktsegment) location \"{locationPath}/tpch/{size}/parquet/partitioned/{size}_parquet/customer/\" options (\"compression\"=\"snappy\")\"\"\"\n",
    "    prt = f\"\"\"create table part (p_partkey long, p_name string, p_mfgr string, p_brand string, p_type string, p_size int, p_container string, p_retailprice decimal(12, 2), p_comment string) using parquet partitioned by (p_brand) location \"{locationPath}/tpch/{size}/parquet/partitioned/{size}_parquet/part/\" options (\"compression\"=\"snappy\")\"\"\"\n",
    "    litem = f\"\"\"create table lineitem (l_orderkey long, l_partkey long, l_suppkey long, l_linenumber int, l_quantity decimal(12, 2), l_extendedprice decimal(12, 2), l_discount decimal(12, 2), l_tax decimal(12,2), l_returnflag string, l_linestatus string, l_shipdate date, l_commitdate date, l_receiptdate date, l_shipinstruct string, l_shipmode string, l_comment string) USING parquet PARTITIONED BY (l_shipdate) LOCATION \"{locationPath}/tpch/{size}/parquet/partitioned/{size}_parquet/lineitem/\" options (\"compression\"=\"snappy\")\"\"\"\n",
    "\n",
    "    for table in tpchtables:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {table}\")\n",
    "\n",
    "    spark.sql(ntn)\n",
    "    spark.sql(rgn)\n",
    "    spark.sql(prtsupp)\n",
    "    spark.sql(supp)\n",
    "    spark.sql(odrs)\n",
    "    spark.sql(cstmr)\n",
    "    spark.sql(prt)\n",
    "    spark.sql(litem)\n",
    "    spark.sql(\"msck repair table orders\")\n",
    "    spark.sql(\"msck repair table customer\")\n",
    "    spark.sql(\"msck repair table part\")\n",
    "    spark.sql(\"msck repair table lineitem\")\n",
    "    \n",
    "    print(\"TPCH Tables set up\")\n",
    "\n",
    "# Set up TPCDS tables\n",
    "def setupTPCDSTables(spark: SparkSession, locationPath: str, size: str):\n",
    "    spark.sparkContext.setJobDescription(\"SETUP\")\n",
    "    tpcdstables = [\"call_center\", \"catalog_page\", \"catalog_returns\", \"catalog_sales\", \"customer\", \"customer_address\", \"customer_demographics\", \"date_dim\", \"household_demographics\", \"income_band\", \"inventory\", \"item\", \"promotion\", \"reason\", \"ship_mode\", \"store\", \"store_returns\", \"store_sales\", \"time_dim\", \"warehouse\", \"web_page\", \"web_returns\", \"web_sales\", \"web_site\"]\n",
    "\n",
    "    print(\"Setting up TPCDS tables\")\n",
    "\n",
    "    store_sales = f\"\"\"create table store_sales using parquet location \"{locationPath}/tpcds/{size}/parquet/partitioned/{size}_parquet/store_sales/\" options (\"compression\"=\"snappy\")\"\"\"\n",
    "    store = f\"\"\"create table store using parquet location \"{locationPath}/tpcds/{size}/parquet/partitioned/{size}_parquet/store/\" options (\"compression\"=\"snappy\")\"\"\"\n",
    "    call_center = f\"\"\"create table call_center using parquet location \"{locationPath}/tpcds/{size}/parquet/partitioned/{size}_parquet/call_center/\" options (\"compression\"=\"snappy\")\"\"\"\n",
    "    catalog_page = f\"\"\"create table catalog_page using parquet location \"{locationPath}/tpcds/{size}/parquet/partitioned/{size}_parquet/catalog_page/\" options (\"compression\"=\"snappy\")\"\"\"\n",
    "    customer = f\"\"\"create table customer using parquet location \"{locationPath}/tpcds/{size}/parquet/partitioned/{size}_parquet/customer/\" options (\"compression\"=\"snappy\")\"\"\"\n",
    "    customer_address = f\"\"\"create table customer_address using parquet location \"{locationPath}/tpcds/{size}/parquet/partitioned/{size}_parquet/customer_address/\" options (\"compression\"=\"snappy\")\"\"\"\n",
    "    customer_demographics = f\"\"\"create table customer_demographics using parquet location \"{locationPath}/tpcds/{size}/parquet/partitioned/{size}_parquet/customer_demographics/\" options (\"compression\"=\"snappy\")\"\"\"\n",
    "    date_dim = f\"\"\"create table date_dim using parquet location \"{locationPath}/tpcds/{size}/parquet/partitioned/{size}_parquet/date_dim/\" options (\"compression\"=\"snappy\")\"\"\"\n",
    "    household_demographics = f\"\"\"create table household_demographics using parquet location \"{locationPath}/tpcds/{size}/parquet/partitioned/{size}_parquet/household_demographics/\" options (\"compression\"=\"snappy\")\"\"\"\n",
    "    income_band = f\"\"\"create table income_band using parquet location \"{locationPath}/tpcds/{size}/parquet/partitioned/{size}_parquet/income_band/\" options (\"compression\"=\"snappy\")\"\"\"\n",
    "    inventory = f\"\"\"create table inventory using parquet location \"{locationPath}/tpcds/{size}/parquet/partitioned/{size}_parquet/inventory/\" options (\"compression\"=\"snappy\")\"\"\"\n",
    "    item = f\"\"\"create table item using parquet location \"{locationPath}/tpcds/{size}/parquet/partitioned/{size}_parquet/item/\" options (\"compression\"=\"snappy\")\"\"\"\n",
    "    promotion = f\"\"\"create table promotion using parquet location \"{locationPath}/tpcds/{size}/parquet/partitioned/{size}_parquet/promotion/\" options (\"compression\"=\"snappy\")\"\"\"\n",
    "    reason = f\"\"\"create table reason using parquet location \"{locationPath}/tpcds/{size}/parquet/partitioned/{size}_parquet/reason/\" options (\"compression\"=\"snappy\")\"\"\"\n",
    "    ship_mode = f\"\"\"create table ship_mode using parquet location \"{locationPath}/tpcds/{size}/parquet/partitioned/{size}_parquet/ship_mode/\" options (\"compression\"=\"snappy\")\"\"\"\n",
    "    store_returns = f\"\"\"create table store_returns using parquet location \"{locationPath}/tpcds/{size}/parquet/partitioned/{size}_parquet/store_returns/\" options (\"compression\"=\"snappy\")\"\"\"\n",
    "    time_dim = f\"\"\"create table time_dim using parquet location \"{locationPath}/tpcds/{size}/parquet/partitioned/{size}_parquet/time_dim/\" options (\"compression\"=\"snappy\")\"\"\"\n",
    "    warehouse = f\"\"\"create table warehouse using parquet location \"{locationPath}/tpcds/{size}/parquet/partitioned/{size}_parquet/warehouse/\" options (\"compression\"=\"snappy\")\"\"\"\n",
    "    web_page = f\"\"\"create table web_page using parquet location \"{locationPath}/tpcds/{size}/parquet/partitioned/{size}_parquet/web_page/\" options (\"compression\"=\"snappy\")\"\"\"\n",
    "    web_returns = f\"\"\"create table web_returns using parquet location \"{locationPath}/tpcds/{size}/parquet/partitioned/{size}_parquet/web_returns/\" options (\"compression\"=\"snappy\")\"\"\"\n",
    "    web_sales = f\"\"\"create table web_sales using parquet location \"{locationPath}/tpcds/{size}/parquet/partitioned/{size}_parquet/web_sales/\" options (\"compression\"=\"snappy\")\"\"\"\n",
    "    web_site = f\"\"\"create table web_site using parquet location \"{locationPath}/tpcds/{size}/parquet/partitioned/{size}_parquet/web_site/\" options (\"compression\"=\"snappy\")\"\"\"\n",
    "    catalog_sales = f\"\"\"create table catalog_sales using parquet location \"{locationPath}/tpcds/{size}/parquet/partitioned/{size}_parquet/catalog_sales/\" options (\"compression\"=\"snappy\")\"\"\"\n",
    "    catalog_returns = f\"\"\"create table catalog_returns using parquet location \"{locationPath}/tpcds/{size}/parquet/partitioned/{size}_parquet/catalog_returns/\" options (\"compression\"=\"snappy\")\"\"\"\n",
    "\n",
    "    for table in tpcdstables:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {table}\")\n",
    "\n",
    "    spark.sql(store_sales)\n",
    "    spark.sql(store)\n",
    "    spark.sql(call_center)\n",
    "    spark.sql(catalog_page)\n",
    "    spark.sql(customer)\n",
    "    spark.sql(customer_address)\n",
    "    spark.sql(customer_demographics)\n",
    "    spark.sql(date_dim)\n",
    "    spark.sql(household_demographics)\n",
    "    spark.sql(income_band)\n",
    "    spark.sql(inventory)\n",
    "    spark.sql(item)\n",
    "    spark.sql(promotion)\n",
    "    spark.sql(reason)\n",
    "    spark.sql(ship_mode)\n",
    "    spark.sql(store_returns)\n",
    "    spark.sql(time_dim)\n",
    "    spark.sql(warehouse)\n",
    "    spark.sql(web_page)\n",
    "    spark.sql(web_returns)\n",
    "    spark.sql(web_sales)\n",
    "    spark.sql(web_site)\n",
    "    spark.sql(catalog_sales)\n",
    "    spark.sql(catalog_returns)\n",
    "    spark.sql(\"alter table store_sales recover partitions\")\n",
    "    spark.sql(\"alter table catalog_sales recover partitions\")\n",
    "    spark.sql(\"alter table web_sales recover partitions\")\n",
    "    spark.sql(\"alter table catalog_returns recover partitions\")\n",
    "    spark.sql(\"alter table store_returns recover partitions\")\n",
    "    spark.sql(\"alter table web_returns recover partitions\")\n",
    "    spark.sql(\"alter table inventory recover partitions\")\n",
    "\n",
    "    print(\"TPCDS Tables set up\")\n",
    "    \n",
    "# Returns list of query names in GCS folder of the given benchmark\n",
    "def getQueryNames(bucket_name: str, tier: str, runtype: str, benchmark: str) -> List[str]:\n",
    "    file_names = []\n",
    "    gcs_query_path = f\"{tier}/{runtype}/notebook_utils/{benchmark}-queries/\"\n",
    "    \n",
    "    try:\n",
    "        client = storage.Client()\n",
    "        bucket = client.get_bucket(bucket_name)\n",
    "        blobs = bucket.list_blobs(prefix=gcs_query_path)      \n",
    "        \n",
    "        for blob in blobs:\n",
    "            if not blob.name.endswith(\"/\"):  \n",
    "                relative_path = os.path.relpath(blob.name, gcs_query_path)\n",
    "                file_name = os.path.basename(relative_path)\n",
    "                name_without_suffix = file_name\n",
    "                dot_index = file_name.rfind('.')\n",
    "                if dot_index > 0:\n",
    "                    name_without_suffix = file_name[:dot_index]\n",
    "                file_names.append(name_without_suffix)\n",
    "\n",
    "        file_names.sort() \n",
    "        return file_names\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error listing files in 'gs://{bucket_name}/{gcs_query_path}': {e}\")\n",
    "        raise ValueError(f\"Failed to get query names from 'gs://{bucket_name}/{gcs_query_path}'\")\n",
    "\n",
    "# Returns the query string from given GCS path\n",
    "def getQuery(spark: SparkSession, gcsPath: str) -> str:\n",
    "    query = \"\"\n",
    "    try:\n",
    "        # Use Spark's textFile to read the content of the GCS file into an RDD\n",
    "        rdd = spark.sparkContext.textFile(gcsPath)\n",
    "        # Join all lines of the RDD with a new line\n",
    "        query = \"\\n\".join(rdd.collect())\n",
    "        return query\n",
    "    except Exception as e:\n",
    "        if \"java.io.FileNotFoundException\" in str(e):\n",
    "            print(f\"Error: File not found at path '{gcsPath}'\")\n",
    "        else:\n",
    "            print(f\"An error occurred while reading '{gcsPath}': {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Measure query runtime in ms\n",
    "def measureTimeMs(query: str) -> int:\n",
    "    try:\n",
    "        start_time = time.perf_counter()\n",
    "        spark.sql(query).collect()\n",
    "        end_time = time.perf_counter()\n",
    "        elapsedTime = end_time - start_time\n",
    "        elapsedTime = elapsedTime * 1000\n",
    "        elapsedTime = round(elapsedTime)\n",
    "        return elapsedTime\n",
    "    except Exception as e:\n",
    "        print(f\"Error running query: {query}\\nError: {e}\")\n",
    "        return -1\n",
    "\n",
    "# Write query runtimes to given GCS path  \n",
    "def write_query_runtimes_to_gcs_csv(\n",
    "    runtime_data: List[Tuple[str, int]],\n",
    "    result_location: str\n",
    "):\n",
    " \n",
    "    if not runtime_data:\n",
    "        print(\"No data to write to CSV.\")\n",
    "        return\n",
    "    \n",
    "    path_without_prefix = result_location[len(\"gs://\"):]\n",
    "    parts = path_without_prefix.split('/', 1)\n",
    "\n",
    "    if len(parts) == 2:\n",
    "        gcs_bucket_name = parts[0]\n",
    "        gcs_file_path = parts[1]\n",
    "    else: \n",
    "        raise ValueError(f\"Invalid result location!\")\n",
    "\n",
    "    # Create an in-memory file-like object\n",
    "    csv_buffer = io.StringIO()\n",
    "\n",
    "    # Create a CSV writer object\n",
    "    csv_writer = csv.writer(csv_buffer)\n",
    "\n",
    "    # Write the header row\n",
    "    csv_writer.writerow([\"Query_Id\", \"Runtime (in ms)\"])\n",
    "\n",
    "    # Write the data rows\n",
    "    for query_id, runtime in runtime_data:\n",
    "        csv_writer.writerow([query_id, runtime])\n",
    "\n",
    "    # Get the CSV data from the buffer\n",
    "    csv_data = csv_buffer.getvalue()\n",
    "\n",
    "    # Initialize the GCS client\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(gcs_bucket_name)\n",
    "    blob = bucket.blob(gcs_file_path)\n",
    "\n",
    "    try:\n",
    "        # Upload the CSV data to GCS\n",
    "        blob.upload_from_string(csv_data, content_type=\"text/csv\")\n",
    "        print(f\"Runtime results successfully written to gs://{gcs_bucket_name}/{gcs_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to GCS: {e}\")\n",
    "\n",
    "# Run TPC benchmark \n",
    "def runTPCBenchmark(spark: SparkSession, benchmark: str, benchmark_configs: dict[str, str], bucket_name:str, run_id: str, project_id: str, runtype: str, tier: str):\n",
    "    size = benchmark_configs.get(\"size\", \"\")\n",
    "    file_format = benchmark_configs.get(\"file_format\", \"\")   \n",
    "    prefix_location_path = f\"gs://{bucket_name}/{tier}/{runtype}\"\n",
    "    query_folder = f\"{prefix_location_path}/notebook_utils\"\n",
    "    query_names = getQueryNames(bucket_name,tier,runtype,benchmark)\n",
    "    result_location = f\"{prefix_location_path}/output/{file_format}/{benchmark}/{size}/{run_id}.csv\"\n",
    "\n",
    "        \n",
    "    if benchmark == \"tpch\":\n",
    "        queries_to_run = benchmark_configs.get(\"TPCH_queries_to_run\", \"\")\n",
    "        print(f\"Running benchmark: {benchmark}, size: {size}, queries to run: {queries_to_run}, runtime engine: {runtype}, file format: {file_format}\")\n",
    "        \n",
    "        setupTPCHTables(spark,prefix_location_path,size)         \n",
    "        \n",
    "        print(\"--------Running TPCH Benchmark with following configurations--------\")\n",
    "        \n",
    "    else:\n",
    "        queries_to_run = benchmark_configs.get(\"TPCDS_queries_to_run\", \"\")\n",
    "        print(f\"Running benchmark: {benchmark}, size: {size}, queries to run: {queries_to_run}, runtime engine: {runtype}, file format: {file_format}\")\n",
    "        \n",
    "        setupTPCDSTables(spark,prefix_location_path,size)\n",
    "        \n",
    "        print(\"--------Running TPCDS Benchmark with following configurations--------\")\n",
    "    \n",
    "    print(f\"Queries to run: {queries_to_run}\")\n",
    "    print(f\"Benchmark: {benchmark}\")\n",
    "    print(f\"Result Location: {result_location}\")\n",
    "    print(f\"Scale Factor: {size}\")\n",
    "    print(f\"File Format: {file_format}\")\n",
    "    \n",
    "    print(f\"Parquet Tables GCS Path: {prefix_location_path}\")\n",
    "            \n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    \n",
    "    runtime: List[Tuple[str, int]] = []\n",
    "\n",
    "    filtered_query_names = []\n",
    "    if queries_to_run.strip() == \"all\":\n",
    "        filtered_query_names = query_names\n",
    "    elif queries_to_run:\n",
    "        queries_to_run_list = [q.strip() for q in queries_to_run.split(',')] # Split by comma if multiple queries\n",
    "        filtered_query_names = [q for q in query_names if q in queries_to_run_list]\n",
    "    \n",
    "    for query_name in filtered_query_names:\n",
    "        query_path = f\"{query_folder}/{benchmark}-queries/{query_name}.sql\"\n",
    "        query = getQuery(spark, query_path)\n",
    "        if query:\n",
    "            print(f\"Running {benchmark} query {query_name}\")\n",
    "            spark.sparkContext.setJobDescription(query_name)\n",
    "            query_run_time = measureTimeMs(query)\n",
    "            if query_run_time != -1:                            \n",
    "                print(f\"{benchmark} query {query_name} ran in {query_run_time} ms\")\n",
    "                runtime.append((query_name, query_run_time))\n",
    "            else:\n",
    "                print(f\"{benchmark} query {query_name} failed.\")\n",
    "                runtime.append((query_name, 0))                \n",
    "        else:\n",
    "            print(f\"Skipping query {query_name} due to reading error.\")\n",
    "    \n",
    "    query_ids = [item[0] for item in runtime]\n",
    "    runtimes = [item[1] for item in runtime]\n",
    "    \n",
    "    headers=[\"QueryId\", \"Runtime(in ms)\"]\n",
    "    total_runtime = sum(runtime_ms for _, runtime_ms in runtime)\n",
    "    runtime.insert(0,(\"Sum\",total_runtime))\n",
    "    \n",
    "    print(tabulate(runtime, headers=headers, tablefmt=\"fancy_grid\"))\n",
    "    \n",
    "    plt.figure(figsize=(25, 15)) \n",
    "    plt.bar(query_ids, runtimes, color='skyblue')\n",
    "    plt.xlabel(\"Query Id\", fontsize=20)\n",
    "    plt.ylabel(\"Runtime (in ms)\", fontsize=20)\n",
    "    plt.title(f\"Query Runtimes for {benchmark} {size} (Runtype: {runtype}, File format: {file_format})\", fontsize=25)\n",
    "    plt.xticks(rotation=90, ha='right',fontsize=15)  \n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.tight_layout() \n",
    "    plt.show()\n",
    "    \n",
    "    print(\"----------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    # Write results to GCS bucket\n",
    "    print(f\"Writing to GCS bucket {bucket_name}\")\n",
    "    write_query_runtimes_to_gcs_csv(runtime, result_location)    \n",
    "    print(\"----------------------------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    spark.stop()\n",
    "    \n",
    "################################################ Terminate Session functions ############################################\n",
    "\n",
    "#  Returns session UUID of the current serverless session\n",
    "def get_session_uuid():\n",
    "    hostname = os.environ.get(\"HOSTNAME\")\n",
    "    if hostname:\n",
    "        pattern = r\"gdpic-srvls-session-([a-zA-Z0-9\\-]+)-m\"\n",
    "        match = re.search(pattern, hostname)\n",
    "        \n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        else:\n",
    "            raise ValueError(f\"Session UUID not found in hostname: '{hostname}'\")\n",
    "\n",
    "# Serialises the session object to json        \n",
    "def serialize_session(obj):\n",
    "    if isinstance(obj, dataproc.Session):\n",
    "        session_dict = {\n",
    "            \"name\": obj.name,\n",
    "            \"creator\": obj.creator,\n",
    "            \"session_template\": obj.session_template,\n",
    "        }\n",
    "        \n",
    "        if hasattr(obj, 'runtime_config'):\n",
    "            runtime_config_data = {\n",
    "                \"version\": obj.runtime_config.version if hasattr(obj.runtime_config, 'version') else None,\n",
    "                \"properties\": []\n",
    "            }\n",
    "            if hasattr(obj.runtime_config, 'properties'):\n",
    "                for prop in obj.runtime_config.properties:\n",
    "                    runtime_config_data[\"properties\"].append({\n",
    "                        \"key\": prop,\n",
    "                        \"value\": obj.runtime_config.properties[prop]\n",
    "                    })\n",
    "            session_dict[\"runtime_config\"] = runtime_config_data\n",
    "\n",
    "        return session_dict\n",
    "    return obj\n",
    "\n",
    "# Upload session details to GCS and terminate session as part of cleanup\n",
    "def finalize_notebook_and_cleanup(benchmark_configs: dict[str, str], gcs_bucket_name: str, region: str, project_id: str, run_id: str, runtype: str, tier: str):\n",
    "    try:\n",
    "        size = benchmark_configs.get(\"size\", \"\")\n",
    "        file_format = benchmark_configs.get(\"file_format\", \"\")\n",
    "        \n",
    "        # Terminating current session \n",
    "        session_uuid = get_session_uuid()\n",
    "    \n",
    "        client_options = {\"api_endpoint\": f\"{region}-dataproc.googleapis.com\"}\n",
    "        client = dataproc.SessionControllerClient(client_options=client_options)\n",
    "\n",
    "        parent = f\"projects/{project_id}/locations/{region}\"\n",
    "        storage_client = storage.Client(project=project_id)\n",
    "        bucket = storage_client.bucket(gcs_bucket_name)\n",
    "        \n",
    "        # GCS Path where session configs is uploaded\n",
    "        gcs_file_path = f\"{tier}/serverless_{runtype}_session/{file_format}/{size}/{run_id}.json\"\n",
    "        blob = bucket.blob(gcs_file_path)\n",
    "        found_session = False\n",
    "        for session in client.list_sessions(parent=parent):\n",
    "            if session.uuid == session_uuid :\n",
    "                found_session = True\n",
    "                print(f\"Current session name: {session.name}\")\n",
    "                \n",
    "                # Uploading session details to GCS                \n",
    "                print(\"Uploading session details to GCS...\")\n",
    "                json_data = json.dumps(session,default=serialize_session, indent=4)\n",
    "                blob.upload_from_string(json_data, content_type=\"application/json\")\n",
    "                \n",
    "                print(f\"Uploaded session details to GCS file path gs://{gcs_bucket_name}/{gcs_file_path}\")\n",
    "                print(\"----------------------------------------------------------------------------------\")\n",
    "                # Terminating current session\n",
    "                print(f\"Terminating session: {session.name}\")\n",
    "                delete_operation = client.terminate_session(name=session.name)\n",
    "                print(f\"Successfully requested to stop session '{session.name}'. The kernel will now terminate...\")\n",
    "                break\n",
    "        else:\n",
    "            if not found_session:\n",
    "                print(\"ERROR! Could not find session details\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while cleaning up current session : {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b17740e-4cfc-45d6-8007-3651c4c7965a",
   "metadata": {},
   "source": [
    "### First Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a11f38c-dedc-477b-8396-acbd9d79a3a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Start Spark Session\n",
    "\n",
    "In the following step, spark properties are set to run benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afd3ce9d-2e9a-4690-8615-6423675d7198",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Application Name: PySparkShell\n",
      "Spark Application ID: app-20250808090813-0001\n",
      "Spark Dataproc Engine: default\n",
      "Spark Lightning Engine: default\n",
      "Spark Session ID: app-20250808090813-0001\n",
      "Spark Version: 3.5.1\n",
      "Spark Master: spark://gdpic-srvls-session-a5ebfa48-e65b-49e1-9c7e-48744a685b42-m:7077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/08 09:13:24 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "#  Start new Spark session with specified configurations \n",
    "\n",
    "spark_configs = {}\n",
    "\n",
    "if(size == \"sf10000\"):\n",
    "    print(\"Killing earlier spark session to update spark config for sf10000 runs\")\n",
    "    spark.stop()    \n",
    "    spark_configs[\"spark.executor.instances\"] = \"27\"\n",
    "\n",
    "spark_builder = SparkSession.builder.appName(f\"{first_benchmark} s8s premium benchmark run\").enableHiveSupport()\n",
    "\n",
    "for key, value in spark_configs.items():\n",
    "    spark_builder.config(key, value)\n",
    "\n",
    "spark = spark_builder.getOrCreate()\n",
    "\n",
    "# Set the logger to display only ERROR logs\n",
    "logger = sc._jvm.org.apache.logging.log4j\n",
    "logger.core.config.Configurator.setAllLevels(\"org\",logger.Level.ERROR)\n",
    "\n",
    "print(f\"Spark Application Name: {spark.sparkContext.appName}\")\n",
    "print(f\"Spark Application ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"Spark Dataproc Engine: {spark.conf.get('spark.dataproc.engine')}\")\n",
    "print(f\"Spark Lightning Engine: {spark.conf.get('spark.dataproc.lightningEngine.runtime')}\")\n",
    "print(f\"Spark Session ID: {spark.sparkContext._jsc.sc().applicationId()}\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark Master: {spark.sparkContext.master}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceef4311-456d-4feb-81fd-fde312df3698",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Run Benchmark\n",
    "The following cell runs the first benchmark with the configurations passed.\n",
    "\n",
    "\n",
    "**NOTE - After every benchmark run is complete, it terminates the existing spark session. New spark session is created for each run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f17e624-8879-497a-a839-fa1d7720cc1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running benchmark: tpch, size: sf1000, queries to run: all, runtime engine: spark, file format: parquet\n",
      "Setting up TPCH tables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/spark/conf/ivysettings.xml will be used\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPCH Tables set up\n",
      "--------Running TPCH Benchmark with following configurations--------\n",
      "Queries to run: all\n",
      "Benchmark: tpch\n",
      "Result Location: gs://sparkpoc-bucket-serverless-premium-279814974066-us-east1/premium/spark/output/parquet/tpch/sf1000/serverless_premium_spark_parquet_sf1000_tpch_tpcds_us-east1_2025-08-08_09-08-18.csv\n",
      "Scale Factor: sf1000\n",
      "File Format: parquet\n",
      "Parquet Tables GCS Path: gs://sparkpoc-bucket-serverless-premium-279814974066-us-east1/premium/spark\n",
      "--------------------------------------------------------------------\n",
      "Running tpch query q1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpch query q1 ran in 120122 ms\n",
      "Running tpch query q10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpch query q10 ran in 39310 ms\n",
      "Running tpch query q11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpch query q11 ran in 14685 ms\n",
      "Running tpch query q12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpch query q12 ran in 28080 ms\n",
      "Running tpch query q13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpch query q13 ran in 45225 ms\n",
      "Running tpch query q14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpch query q14 ran in 6831 ms\n",
      "Running tpch query q15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpch query q15 ran in 16137 ms\n",
      "Running tpch query q16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpch query q16 ran in 19281 ms\n",
      "Running tpch query q17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpch query q17 ran in 31485 ms\n",
      "Running tpch query q18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 173:========>                                         (160 + 160) / 1000]8]\r"
     ]
    }
   ],
   "source": [
    "# Run first benchmark\n",
    "runTPCBenchmark(spark,first_benchmark,benchmark_configs,BUCKET_NAME,RUN_ID,PROJECT,RUNTYPE,TIER)\n",
    "\n",
    "if second_benchmark is None:\n",
    "    # Call cleanup function\n",
    "    finalize_notebook_and_cleanup(benchmark_configs,BUCKET_NAME,REGION,PROJECT,RUN_ID,RUNTYPE,TIER)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b528ea-110b-4059-b2fb-f859b502ffec",
   "metadata": {},
   "source": [
    "### Second Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8b638a-7614-4e5a-94f3-5f9592819bce",
   "metadata": {},
   "source": [
    "#### Start Spark Session\n",
    "\n",
    "In the following step, spark properties are set to run benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf029b5e-049f-42c4-b1f1-4ec70bfe862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if second_benchmark is not None: \n",
    "    #  Start new Spark session with specified configurations \n",
    "    \n",
    "    spark_configs = {}\n",
    "    \n",
    "    if(size == \"sf10000\"):\n",
    "        spark_configs[\"spark.executor.instances\"] = \"27\"\n",
    "    \n",
    "    spark_builder = SparkSession.builder.appName(f\"{second_benchmark} s8s premium benchmark run\").enableHiveSupport()\n",
    "    \n",
    "    for key, value in spark_configs.items():\n",
    "        spark_builder.config(key, value)\n",
    "    \n",
    "    spark = spark_builder.getOrCreate()\n",
    "\n",
    "    # Set the logger to display only ERROR logs\n",
    "    logger = sc._jvm.org.apache.logging.log4j\n",
    "    logger.core.config.Configurator.setAllLevels(\"org\",logger.Level.ERROR)\n",
    "\n",
    "    print(f\"Spark Application Name: {spark.sparkContext.appName}\")\n",
    "    print(f\"Spark Application ID: {spark.sparkContext.applicationId}\")\n",
    "    print(f\"Spark Session ID: {spark.sparkContext._jsc.sc().applicationId()}\")\n",
    "    print(f\"Spark Version: {spark.version}\")\n",
    "    print(f\"Spark Master: {spark.sparkContext.master}\")\n",
    "\n",
    "else:\n",
    "    print(\"Second benchmark is empty!\")\n",
    "    print(\"Skipping this step.\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72fa236-c9fd-4d31-a9d7-1bfd14d25795",
   "metadata": {},
   "source": [
    "#### Run Benchmark\n",
    "The following cell runs the second benchmark with the configurations passed.\n",
    "\n",
    "\n",
    "**NOTE - After every benchmark run is complete, it terminates the existing spark session. New spark session is created for each run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a377c564-2193-4e43-86df-c49a8dddc216",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1483:==========================>                         (86 + 81) / 167]\r"
     ]
    }
   ],
   "source": [
    "if second_benchmark is not None:   \n",
    "    \n",
    "    # Run second benchmark\n",
    "    runTPCBenchmark(spark,second_benchmark,benchmark_configs,BUCKET_NAME,RUN_ID,PROJECT,RUNTYPE,TIER)\n",
    "    \n",
    "    # Call cleanup function\n",
    "    finalize_notebook_and_cleanup(benchmark_configs,BUCKET_NAME,REGION,PROJECT,RUN_ID,RUNTYPE,TIER)\n",
    "else:\n",
    "    print(\"Second benchmark is empty!\")\n",
    "    print(\"Skipping this step.\")        "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "9c39b79e5d2e7072beb4bd59-premium-spark-runtime",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Premium for running TPC Benchmark on Serverless Spark (Remote)",
   "language": "python",
   "name": "9c39b79e5d2e7072beb4bd59-premium-spark-runtime"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
